{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b1327b3",
   "metadata": {},
   "source": [
    "# Assignment 10 - Bonus assignments\n",
    "\n",
    "## Part 1 - Introduction to Deep learning\n",
    "\n",
    "Even though Sklearn has some neural network tools built-in, the most common Python libraries for deep learning and neural networks is Keras, Tensorflow or Pytorch. In this task, we will create a neural network using Keras. \n",
    "\n",
    "We will keep the same typ of network as in **Assignment 5**, a Multi Layer Perceptron.\n",
    "\n",
    "### 1. Required libraries\n",
    "\n",
    "These are the required libraries. You will have to install Keras and Tensorflow by running `pip install keras` and `pip install tensorflow`. \n",
    "\n",
    "You can create a separate Anaconda environment on your computer specific for this assignemnt, this is not required but can speed up the installation and development. Red more on how to create an environment here: [Manage Anaconda Environments](https://conda.io/projects/conda/en/latest/user-guide/tasks/manage-environments.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65b5e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import RMSprop\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a839cbc6",
   "metadata": {},
   "source": [
    "### 2. Starter functions\n",
    "\n",
    "Here are two functions from **Assignment 5** that might be useful for you. \n",
    "\n",
    "First is the function for showing an array of pixel values as an image using `matplotlib`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ace6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will reshape and plot an image from a pixel value array\n",
    "def plot_image(num_plots, plot_index, plot_image, pixel_hw, title_text, colors=\"gray\"):\n",
    "    plt.subplot(1, n_plots, plot_index)\n",
    "    reshaped_image = plot_image.reshape(pixel_hw, pixel_hw)\n",
    "    plt.imshow(reshaped_image, cmap=colors)\n",
    "    plt.title(title_text, size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db608b75",
   "metadata": {},
   "source": [
    "Then there is the function for displaying a confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b49d5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_confusion_matrix(y_true, y_pred, num_classes, title='Confusion matrix'):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(5,5))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap='Blues')\n",
    "    plt.title(title, size=15)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(num_classes)\n",
    "    plt.xticks(tick_marks, [str(i) for i in range(num_classes)], rotation=45, size=10)\n",
    "    plt.yticks(tick_marks, [str(i) for i in range(num_classes)], size=10)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('Actual label', size=15)\n",
    "    plt.xlabel('Predicted label', size=15)\n",
    "    width, height = cm.shape\n",
    "    for x in range(width):\n",
    "        for y in range(height):\n",
    "            plt.annotate(\n",
    "                str(cm[x][y]), xy=(y, x), \n",
    "                horizontalalignment='center',\n",
    "                verticalalignment='center'\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b50724",
   "metadata": {},
   "source": [
    "### 3. Loading the data\n",
    "\n",
    "Load the data in the same way as in **Assignment 5** by using `pd.read_csv()` to read the values from both the training dataset and the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06256d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3214c461",
   "metadata": {},
   "source": [
    "### 4. Split data into features, labels, training and testing\n",
    "\n",
    "Again, just as in **Assignment 5**, split the data into training data and test data as well as features and labels and name them something like `X_train`, `X_test`, `y_train` and `y_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87382c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f229b0",
   "metadata": {},
   "source": [
    "### 5. Preparing the data\n",
    "\n",
    "To train the model from tensorflow we need to convert the labels from a one dimensional array to an array of one-hot encoded vectors. This can be done using the function `to_categorical()`. Here's how it works:\n",
    "\n",
    "1. **Input**: The function takes an array-like object containing integer labels as its input. These labels represent the categories or classes for each sample in your dataset.\n",
    "\n",
    "2. **Encoding**: The function converts the integer labels into a binary matrix representation known as one-hot encoding. One-hot encoding represents each category as a binary vector where only one element is 1 (hot) and the rest are 0 (cold).\n",
    "\n",
    "3. **Output**: The function returns a numpy array or a sparse matrix containing the one-hot encoded vectors corresponding to the input labels.\n",
    "\n",
    "Here's a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a570e67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = np.array([0, 1, 2, 1, 0])  # Integer labels\n",
    "\n",
    "# Convert integer labels to one-hot encoded vectors\n",
    "one_hot_labels = to_categorical(labels)\n",
    "\n",
    "print(one_hot_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607c8f3c",
   "metadata": {},
   "source": [
    "The resulting array of arrays is like a table where each column represents one label. \n",
    "\n",
    "Do this with your training labels and test labels (`y_train` and `y_test`) and save the result to a new variable for the training labels (you can name it `y_train_one_hot`) and another for the test labels (you can name it something like `y_test_one_hot`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e3c3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a23df1c",
   "metadata": {},
   "source": [
    "Another thing we have to do to prepare our data is to scale the features using `StandardScaler()`. Do just as in **Assignment 5** to scale `X_train` and `X_test` and save both of them to a new variable (you can name them `X_train_scaled` and `X_test_scaled`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f0bb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a988381",
   "metadata": {},
   "source": [
    "### 6. Building our Neural Network\n",
    "\n",
    "The following function builds the neural tetwork and returns it. See the comments for an explanation of each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1491a6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_network():\n",
    "    \n",
    "    # Define our network structure, in this case we want a fully connected neural\n",
    "    # network where we add layers sequentially. For this purpuse, using the Sequantial()\n",
    "    # method is sutiable.\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Add the first layer to our model. Using Dense() will define this layer as a fully \n",
    "    # connected layer where all neurons in this layer are connected to all neurons in \n",
    "    # the previous layer. We chose to put 512 neurons in this layer, and set the\n",
    "    # activation function `relu``. We also need to specify our input shape. Since our\n",
    "    # images are 28x28 pixels, we set the input shape to 784.\n",
    "    model.add(Dense(512, activation='relu', input_shape=(784,)))\n",
    "    \n",
    "    # add a dropout layer, which is a way to randomly select neurons to ignore during\n",
    "    # training. Using dropout is a way to try to reduce overfit. Here we set 20% of the\n",
    "    # neurons to be ignored.\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    # add a \"hidden layer\" to our model, which is another fully connected layer. We set\n",
    "    # the number of neurons to 512 and chose the same activation function ReLu. \n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    \n",
    "    # add another dropput of 20% \n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    # Add the final layer, which is also going to be a fully connected `Dense` layer.\n",
    "    # Since we have 10 classes (10 digits), we use 10 neurons in this layer. \n",
    "    # Since this is a multi class classification problem, the activation function \n",
    "    # `softmax` is a good choise for this purpuse. \n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "model = neural_network()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b5d426",
   "metadata": {},
   "source": [
    "### 7. Model configuration\n",
    "\n",
    "This code configures the compilation step of our model.\n",
    "\n",
    "**Here's an explanation:**\n",
    "\n",
    "- `loss='categorical_crossentropy'`: Loss function determines how the model measures the difference between the predicted outputs and the true outputs. In this case, 'categorical_crossentropy' is a loss function specifically designed for multi-class classification problems. It quantifies the dissimilarity between the predicted probability distribution and the true one-hot encoded labels. The model aims to minimize this loss during training.\n",
    "- `optimizer=RMSprop()`: An optimizer is responsible for adjusting the model's parameters based on the computed gradients during training. RMSprop is one type of optimizer. It adapts the learning rate (how much the parameters are updated) based on the historical gradients. This allows the optimizer to automatically adjust the learning rate for each parameter individually, which can improve convergence and learning speed.\n",
    "- `metrics=['accuracy']`: Metrics are used to evaluate the performance of the model. Accuracy is a commonly used metric for classification tasks. It measures the percentage of correctly predicted samples out of the total number of samples. For example, if the model predicts the correct class for 80 out of 100 samples, the accuracy would be 80%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c885cc34",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "                 optimizer=RMSprop(),\n",
    "                 metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4942a0ad",
   "metadata": {},
   "source": [
    "### 8. Training the model\n",
    "\n",
    "After compiling the model with the configurations above, we can proceed to train the model using the fit method, passing in the training data and labels. During training, the model will minimize the specified loss function using the chosen optimizer, while monitoring the accuracy metric.\n",
    "\n",
    "Train your model by calling its `fit()`-function using the following parameters: \n",
    "- `x`: The scaled features from the training dataset\n",
    "- `y`: The one-hot encoded labels from the training dataset\n",
    "- `batch_size`: During training, the dataset is divided into smaller subsets called batches. The batch size determines the number of samples that will be propagated through the model at once. Set this to 32, which means that the model will process 32 samples at a time before updating the weights based on the computed gradients.\n",
    "- `epochs`: An epoch refers to a complete pass through the entire dataset during training. In other words, it represents the number of times the model will iterate over the entire training dataset. Se this to 10 so that the training process will go through 10 epochs.\n",
    "- `verbose`: Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch. Set this to 1 to display progress bars during training. \n",
    "\n",
    "All arguments and more information about the model can be found at [Keras Sequential fit](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential#fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f10c568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39be61fa",
   "metadata": {},
   "source": [
    "### 9. Making predictions\n",
    "\n",
    "Use the models `predict()`-function ([More information here](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential#predict)) to make predictions. Remember to pass in the scaled test features to the function and save the result in a variable so that you can evaluate your model.\n",
    "\n",
    "The `predict()`-funciton of the model we are using will return a one-hot encoded array of arrays where the values `[0, 1, 2]` will be represented as this array of arrays:\n",
    "\n",
    "```python\n",
    "[[1. 0. 0.]\n",
    " [0. 1. 0.]\n",
    " [0. 0. 1.]]\n",
    "```\n",
    "\n",
    "To convert the one-hot encoded array to a regular array of labels we can use `np.argmax()` to retrieve the index of the maximum value along the specified axis. Since the one-hot encoding represents the categories as binary vectors with a single 1 (hot) and the rest as 0 (cold), `np.argmax()` will return the index of the 1 for each sample, effectively converting it back to a one-dimensional array of labels.\n",
    "\n",
    "Reda more about the `np.argmax()`-function here: [Numpy argmax](https://numpy.org/doc/stable/reference/generated/numpy.argmax.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bd859e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here\n",
    "\n",
    "y_pred_nn_one_hot = model.predict(X_test_scaled)\n",
    "y_pred_nn = np.argmax(y_pred_nn_one_hot, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef7476f",
   "metadata": {},
   "source": [
    "### 10. Evaluating and visualizing\n",
    "\n",
    "Just as in **Assignment 5**:\n",
    "- Use the `accuracy_score()`-function to find out the accuracy of your model\n",
    "- Use `plot_image()` within a loop to show a random selection of the images. Remember to get the correct label and predicted label and set the colormap to red or green based on if the prediction was correct or not.\n",
    "- Use `create_confusion_matrix()` to display which numbers your model had a harder time predicting-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e63a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred_nn)\n",
    "print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "n_plots = 8\n",
    "pixel_height_width = 28\n",
    "\n",
    "for i in range(n_plots):\n",
    "    random_index = random.randint(0, X_test.shape[0])\n",
    "    current_image = X_test[random_index,:]\n",
    "    actual = y_test[random_index]\n",
    "    predicted = y_pred_nn[random_index]\n",
    "    colors = \"Greens\" if (actual == predicted) else \"Reds\"\n",
    "    image_title = f\"true:{actual} pred:{predicted}\"\n",
    "    plot_image(n_plots, i+1, current_image, pixel_height_width, image_title, colors)\n",
    "    \n",
    "create_confusion_matrix(y_test, y_pred_nn, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a0af47",
   "metadata": {},
   "source": [
    "## Part 2 - Another dataset: MNIST fashion\n",
    "\n",
    "Try yourself with another dataset \"fashion mnist\", where the images contain clothes intead of digits. It can be downloaded here: https://www.kaggle.com/datasets/zalando-research/fashionmnist\n",
    "\n",
    "Since the shapes are more complicated to predict than hand digits, you might need to tune your models a bit more this time, and also increase the training time such as number of epochs.\n",
    "\n",
    "Since this is the last part of the last assignment you'll get very little guidance in here.\n",
    "\n",
    "### 1. Load the data\n",
    "\n",
    "Download the dataset from the link above and load it in the same way as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d64eed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here\n",
    "\n",
    "df_train = pd.read_csv('A5_fashion-mnist_train.csv')\n",
    "df_test = pd.read_csv('A5_fashion-mnist_test.csv')\n",
    "\n",
    "df_train.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbbaae8",
   "metadata": {},
   "source": [
    "### 2. Explore the data\n",
    "\n",
    "Since this is new, unknown data, I suggest that you explore it using the techniques we have used in previous assignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917949e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here\n",
    "\n",
    "print(df_train.info(), end='\\n\\n')\n",
    "\n",
    "unique_labels = df_train['label'].unique()\n",
    "sorted_labels = np.sort(unique_labels)\n",
    "print(sorted_labels)\n",
    "\n",
    "label_summary = df_train['label'].describe()\n",
    "print(label_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1671c705",
   "metadata": {},
   "source": [
    "### 3. Preparing the data\n",
    "\n",
    "Just as before, do the following to prepare your data:\n",
    "- Split the data into features and labels for both the training data and test data.\n",
    "- Convert the labels of both the training dataset and the testing dataset to a one-hot encoded array.\n",
    "- Scale the features of the training and testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf5cf96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here\n",
    "\n",
    "# Split data\n",
    "X_train = np.array(df_train.iloc[:, 1:])\n",
    "y_train = np.array(df_train.iloc[:, 0])\n",
    "X_test =  np.array(df_test.iloc[:, 1:])\n",
    "y_test = np.array(df_test.iloc[:, 0])\n",
    "\n",
    "# Convert labels to one-hot encoded\n",
    "y_train_one_hot = to_categorical(y_train)\n",
    "y_test_one_hot = to_categorical(y_test)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546b63ef",
   "metadata": {},
   "source": [
    "Now you can use a loop to display a few random samples from the dataset and get a feel for what type of images it cointains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d1f799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here\n",
    "\n",
    "for i in range(n_plots):\n",
    "    random_index = random.randint(0, X_test.shape[0])\n",
    "    current_image = X_test[random_index,:]\n",
    "    label = y_test[random_index]\n",
    "    image_title = f\"{label}\"\n",
    "    plot_image(n_plots, i+1, current_image, pixel_height_width, image_title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221a4d18",
   "metadata": {},
   "source": [
    "### 4. Preparing the model\n",
    "\n",
    "Create a new instance of a neural network by using the `neural_network()`-function from earlier and compile it like before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c39269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here\n",
    "\n",
    "model = neural_network()\n",
    "model.summary()\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "                 optimizer=RMSprop(),\n",
    "                 metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c519cd",
   "metadata": {},
   "source": [
    "### 5. Training and making predictions\n",
    "\n",
    "Train the model and make predictions on the testing dataset by using the scaled features you just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da85307f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here\n",
    "\n",
    "model.fit(\n",
    "    X_train_scaled,\n",
    "    y_train_one_hot,\n",
    "    batch_size=32,\n",
    "    epochs=3,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "y_pred_nn_one_hot = model.predict(X_test_scaled)\n",
    "y_pred_nn = np.argmax(y_pred_nn_one_hot, axis=1)\n",
    "\n",
    "print('Predictions complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74096f0",
   "metadata": {},
   "source": [
    "### 6. Results\n",
    "\n",
    "Finally you can compare the predictions to the correct labels and print the accuracy and a few random red/green sample images based on the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a925b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred_nn)\n",
    "print(f\"Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "n_plots = 8\n",
    "pixel_height_width = 28\n",
    "\n",
    "for i in range(n_plots):\n",
    "    random_index = random.randint(0, X_test.shape[0])\n",
    "    current_image = X_test[random_index,:]\n",
    "    actual = y_test[random_index]\n",
    "    predicted = y_pred_nn[random_index]\n",
    "    colors = \"Greens\" if (actual == predicted) else \"Reds\"\n",
    "    image_title = f\"true:{actual} pred:{predicted}\"\n",
    "    plot_image(n_plots, i+1, current_image, pixel_height_width, image_title, colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0d4153",
   "metadata": {},
   "source": [
    "You can, one last time, display the confusion matrix for your results by using the same function as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d85e1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here\n",
    "\n",
    "create_confusion_matrix(y_test, y_pred_nn, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6118e57",
   "metadata": {},
   "source": [
    "# Done!\n",
    "\n",
    "Great job this far! I you completed the tasks above you are more than done with this course. YOu don't have to submit the bonus assignments on to the teacher on ItsLearning but I recommend you to push the notebook to GitHub for later reference.\n",
    "\n",
    "**Thank you again for attending this course and good luck with your future Programming and Machine Learning**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
