{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34ea0e97",
   "metadata": {},
   "source": [
    "# Assignment 4\n",
    "\n",
    "Welcome to your machine learning assignment! In this assignment, we will explore the fascinating world of unsupervised learning and delve into the K-means clustering algorithm. We will be using Python and the scikit-learn library to implement and evaluate the performance of the K-means clustering algorithm.\n",
    "\n",
    "In this assignment, I will provide you with a dataset containing various data points. We will use this dataset to identify clusters within the data using the K-means algorithm. By grouping similar data points together, K-means clustering allows us to gain insights and discover patterns in the data.\n",
    "\n",
    "Once we have implemented the K-means algorithm and obtained the cluster labels and cluster centers, we will evaluate the performance of the clustering algorithm. We will use the silhouette score, a widely used metric for unsupervised clustering evaluation, to assess the quality of the identified clusters.\n",
    "\n",
    "Furthermore, we will visualize the data and the identified clusters, allowing us to gain a better understanding of the clustering results and how the data points are grouped together.\n",
    "\n",
    "Are you ready? Let's dive into the exciting world of K-means clustering and unleash the power of unsupervised learning!\n",
    "\n",
    "The dataset for my example can be found here: [Mall Customer Segmentation Data](https://www.kaggle.com/datasets/vjchoudhary7/customer-segmentation-tutorial-in-python)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4ff2e5",
   "metadata": {},
   "source": [
    "## Part 1 - How it's done\n",
    "\n",
    "In this part I will demonstrate how to train a model on existing data by loading the dataset, extract the relevant columns for clustering, which include 'Age', 'Annual Income (k$)', and 'Spending Score (1-100)'.\n",
    "\n",
    "I generate an elbow plot for finding the right amount of clusters, here we can see that 5 clusters is a fitting number since that's where the plot starts to flatten out. I then initialize the K-means clustering algorithm with n_clusters=5, indicating that we want to identify 5 different customer segments.\n",
    "\n",
    "After fitting the model to the data, I obtain the predicted labels and cluster centers. The labels indicate which segment each customer belongs to, and the centers represent the centroids of each cluster.\n",
    "\n",
    "I visualize the data and the identified clusters using a scatter plot. Each customer is represented by their 'Annual Income (k$)' and 'Spending Score (1-100)', and colored according to their assigned cluster. The cluster centers are marked with red crosses.\n",
    "\n",
    "By running this code, you can observe how the K-means algorithm segments customers based on their characteristics and buying patterns. Feel free to customize the code further to suit your specific needs or explore different aspects of customer segmentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b900ca",
   "metadata": {},
   "source": [
    "### 1. Import the required libraries\n",
    "\n",
    "First, I import the necessary libraries and modules that will be used in the code:\n",
    "\n",
    "- `numpy` is imported and aliased as `np` to provide functions for mathematical operations and array manipulations. It is used, for example, to generate sample data and perform computations in the code.\n",
    "- `matplotlib.pyplot` module is imported and aliased as `plt` to create various types of plots, such as line plots, scatter plots, and bar plots. It is used to generate the elbow plot and visualize the data and clusters.\n",
    "- `KMeans` is a class in the `sklearn.cluster` module that implements the K-means clustering algorithm. By importing `KMeans`, we can create an instance of the K-means clustering algorithm to cluster data points.\n",
    "- `pandas` is imported and aliased as `pd` to provide functions and data structures to work with tabular data, such as CSV files. It is used to read the customer data from a CSV file and extract the relevant columns for clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825bc701",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abc15a5",
   "metadata": {},
   "source": [
    "### 2. Read the customer data from a CSV file\n",
    "\n",
    "Here I am reading a CSV file named 'A4_Mall_Customers.csv' using the `pd.read_csv()` function from the pandas library.\n",
    "\n",
    "By using `pd.read_csv()`, I can load the contents of the CSV file into a pandas `DataFrame` object named `df`. This DataFrame allows me to easily manipulate and analyze the data within the CSV file.\n",
    "\n",
    "The CSV file need to be located in the same directory as the code file. If it is located elsewhere, I would need to provide the correct path to the file.\n",
    "\n",
    "Once the CSV file is read and stored in the DataFrame `df`, I can perform various operations on the data, such as data cleaning, exploration, visualization, or applying machine learning algorithms for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caecf512",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('A4_Mall_Customers.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7ef987",
   "metadata": {},
   "source": [
    "### 3. Extract the relevant columns for clustering\n",
    "\n",
    "Now I am accessing specific columns of the DataFrame `df` and storing them in a new DataFrame named X.\n",
    "\n",
    "Using the double square brackets `[['Age', 'Annual Income (k$)', 'Spending Score (1-100)']]`, I am selecting the columns 'Age', 'Annual Income (k$)', and 'Spending Score (1-100)' from the original DataFrame `df`.\n",
    "\n",
    "By doing this, I am creating a subset of the original DataFrame that contains only the selected columns. This can be useful when we want to focus on specific features or variables for further analysis or modeling.\n",
    "\n",
    "The resulting DataFrame `X` will have the same number of rows as the original DataFrame, but it will only contain the columns 'Age', 'Annual Income (k$)', and 'Spending Score (1-100)'. This subset of columns can be used for various purposes, such as clustering, regression, or any other analysis that requires working with specific features of the data.\n",
    "\n",
    "By executing this code, I am creating a new DataFrame `X` that contains the desired columns from the original DataFrame `df`, allowing me to work with a more focused set of features for subsequent steps in my analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fa4efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['Age', 'Annual Income (k$)', 'Spending Score (1-100)']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78f85f5",
   "metadata": {},
   "source": [
    "### 4. Scale the dataset using StandardScaler\n",
    "\n",
    "Scaling the dataset with StandardScaler ensures that the features have similar scales, which helps the K-means algorithm perform better and converge faster. It is important to apply the same scaling to new data points before using the trained K-means model for classification. To do this we follow these steps:\n",
    "\n",
    "1. Create an instance of the StandardScaler class.\n",
    "2. Fit the scaler on our dataset.\n",
    "3. Transform the dataset using the fitted scaler, this will scale the features of our dataset based on the mean and standard deviation calculated during the fit step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382afe01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# 2\n",
    "scaler.fit(X)\n",
    "\n",
    "# 3\n",
    "X_scaled = scaler.transform(X)\n",
    "\n",
    "\n",
    "# Output our results\n",
    "print(\"Shape of X_scaled:\", X_scaled.shape)\n",
    "print(\"Mean of each feature in X_scaled:\", X_scaled.mean(axis=0))\n",
    "print(\"Standard deviation of each feature in X_scaled:\", X_scaled.std(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3a603b",
   "metadata": {},
   "source": [
    "### 5. Principal Component Analysis\n",
    "\n",
    "Principal Component Analysis (PCA) is a dimensionality reduction technique used to simplify complex data and helps make the data easier to work with by transforming it into a new set of variables called principal components. Sometimes, data can have many different measurements or features, which can be overwhelming. PCA simplifies the data by finding the most important patterns or directions in the data.\n",
    "\n",
    "The K-means algorithm looks for groups or clusters in the data. PCA can help by rearranging the data in a way that makes these clusters more distinct and easier to find. It finds the directions where the data varies the most and aligns them with the most important patterns.\n",
    "\n",
    "When we use PCA we simplify and clean the data, making it easier to understand and find patterns. This helps in identifying distinct groups or clusters in the data, even if we have limited knowledge about machine learning.\n",
    "\n",
    "The code below does the following:\n",
    "\n",
    "1. Create an instance of the PCA class, specifying 2 as the number of variables / principal components.\n",
    "2. Fit the PCA-object on our dataset.\n",
    "3. Transform the dataset using the fitted PCA-object with our scaled data, this will simplify the data by finding the most important patterns in the data.\n",
    "\n",
    "*Step 2 and 3 can be combined by using `pca.fit_transform(X_scaled)`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626bc113",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# 2\n",
    "pca.fit(X_scaled)\n",
    "\n",
    "# 3\n",
    "X_pca = pca.transform(X_scaled)\n",
    "\n",
    "# Output the shape of the transformed data\n",
    "X_pca.shape\n",
    "\n",
    "X_pca"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff1fd0f",
   "metadata": {},
   "source": [
    "### 6. Generate elbow plot to find the right amount of clusters\n",
    "\n",
    "I am performing K-means clustering on the data stored in the DataFrame `X` to determine the optimal number of clusters using the elbow method. I then plot an elbow curve to visualize the relationship between the number of clusters and the inertia.\n",
    "\n",
    "I first initialize an empty list called `inertia_values`. This list will be used to store the inertia (sum of squared distances to the nearest centroid) for different values of `k`.\n",
    "\n",
    "Afterwards, I create a range of values for `k` using `range(1, 11)`. This range represents the number of clusters that will be tested.\n",
    "\n",
    "I then enter a `for` loop to iterate over each value of k. For each iteration, I perform the following steps:\n",
    "\n",
    "1. Initialize the K-means clustering algorithm using `KMeans(n_clusters=k, random_state=0)`. This creates an instance of the KMeans class with the specified number of clusters (`k`) and a random seed value of 0 for reproducibility.\n",
    "\n",
    "2. Fit the K-means model to the data using `kmeans.fit(X)`. This step calculates the cluster centers and assigns data points to their nearest centroids based on the specified `k` value.\n",
    "\n",
    "3. Compute the inertia of the K-means model using `kmeans.inertia_`, which represents the sum of squared distances of samples to their closest cluster center.\n",
    "\n",
    "4. Append the inertia value to the `inertia_values` list using `inertia_values.append(kmeans.inertia_)`.\n",
    "\n",
    "After the loop finishes, I plot the elbow curve using `plt.plot()` with k_values on the x-axis and inertia_values on the y-axis. This curve helps visualize the inertia values for different numbers of clusters.\n",
    "\n",
    "I add labels and a title to the plot using `plt.title()`, `plt.xlabel()`, and `plt.ylabel()`. I display the plot using `plt.show()`.\n",
    "\n",
    "So, the code will iterate over different values of k, fit the K-means model to the data, compute the inertia values, and generate an elbow plot. This allows me to determine the optimal number of clusters for customer segmentation based on the elbow point where the inertia values start to level off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc36608",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list to store the inertia values\n",
    "inertia_values = []\n",
    "\n",
    "# Try different values of k\n",
    "k_values = range(1, 15)\n",
    "for k in k_values:\n",
    "    # Initialize K-means clustering algorithm\n",
    "    kmeans = KMeans(n_clusters=k, random_state=0, n_init=10)\n",
    "    \n",
    "    # Fit the model to the data\n",
    "    kmeans.fit(X_pca)\n",
    "    \n",
    "    # Get the inertia (sum of squared distances to the nearest centroid)\n",
    "    inertia_values.append(kmeans.inertia_)\n",
    "    \n",
    "# Plot the elbow curve\n",
    "plt.plot(k_values, inertia_values, marker='o')\n",
    "plt.title('Elbow Plot (Customer Segmentation)')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Inertia')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bbddb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's save the number of clusters we want as K to use for later.\n",
    "# Based on the elbow graph, 4 seems like a good fit\n",
    "# If your choice here gives a bad result you can just \n",
    "# change this number and run the code again from here\n",
    "K = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebc25b9",
   "metadata": {},
   "source": [
    "### 7. Initialize K-means clustering algorithm\n",
    "\n",
    "In this code, I am initializing the K-means clustering algorithm with 4 clusters using the `KMeans` class from scikit-learn.\n",
    "\n",
    "I create an instance of the `KMeans` class and assign it to the variable `kmeans`. The `n_clusters` parameter is set to 5, indicating that I want to divide the data into 5 clusters.\n",
    "\n",
    "Additionally, I set the `random_state` parameter to 0 to ensure reproducibility of the results.\n",
    "\n",
    "The K-means algorithm is typically run multiple times with different centroid seeds (random initial positions of the cluster). The parameter `n_init` specifies the number of runs to perform. Among these runs, the one with the best result is selected. The standasd value to start with here is 10.\n",
    "\n",
    "The code will create a K-means clustering algorithm object (`kmeans`) ready to be fitted to the data to identify the 5 clusters based on their characteristics and patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef3644b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=K, random_state=0, n_init=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08b293a",
   "metadata": {},
   "source": [
    "### 8. Fit the model to the data\n",
    "\n",
    "Here I am fitting the K-means clustering algorithm to the data stored in the DataFrame `X` using the `fit()` method of the `kmeans` object.\n",
    "\n",
    "By calling `kmeans.fit(X)`, I am training the K-means model on the data `X`. The algorithm will calculate the cluster centers and assign each data point to its nearest centroid based on the specified number of clusters and the features in `X`.\n",
    "\n",
    "This step is crucial as it performs the actual clustering of the data. The algorithm iteratively adjusts the cluster centers to minimize the inertia (sum of squared distances to the nearest centroid) until convergence.\n",
    "\n",
    "After this code is executed, the K-means model has learned the cluster centers based on the data and is ready to make predictions or perform further analysis using the trained clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a779a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.fit(X_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a89cca8",
   "metadata": {},
   "source": [
    "### 9. Get the predicted labels and cluster centers\n",
    "\n",
    "Below I am using the fitted K-means clustering algorithm (kmeans) to obtain the cluster labels and cluster centers.\n",
    "\n",
    "By calling `kmeans.labels_`, I am retrieving the cluster labels for each data point in the DataFrame `X`. The resulting labels array will contain an assigned cluster label for each data point, indicating which cluster that data point belongs to.\n",
    "\n",
    "Similarly, by accessing `kmeans.cluster_centers_`, I am obtaining the coordinates of the cluster centers for each of the clusters identified by the K-means algorithm. The resulting `centers` array will contain the centroid coordinates for each cluster.\n",
    "\n",
    "These cluster labels and cluster centers can be used for various purposes. The cluster labels can help in understanding the assignment of data points to different clusters, while the cluster centers provide insight into the representative characteristics of each cluster.\n",
    "\n",
    "Now I can access the cluster labels and cluster centers obtained from the fitted K-means model, allowing me to further analyze and interpret the results of the clustering algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858cbf26",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = kmeans.labels_\n",
    "centers = kmeans.cluster_centers_\n",
    "\n",
    "centers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea97d343",
   "metadata": {},
   "source": [
    "### 10. Visualize the data and clusters\n",
    "\n",
    "Here I am creating a scatter plot to visualize the customer segmentation results using the data from the DataFrame `X`, the cluster labels (`labels`), and the cluster centers (`centers`).\n",
    "\n",
    "Using `plt.scatter()`, I plot the data points from `X_pca` on the `scatter` plot. \n",
    "\n",
    "I also use `plt.scatter()` to plot the cluster centers on the scatter plot. The x-coordinate of the centers is taken from `centers[:, 0]` and `centers[:, 1]`. The c parameter is set to 'red' to color the cluster centers as red, and the marker parameter is set to 'X' to mark the cluster centers with an 'X' symbol.\n",
    "\n",
    "This code visualizes the customer segmentation results on a scatter plot, where each data point is colored according to its assigned cluster label, and the cluster centers are marked with red 'X' symbols. This visualization helps in understanding the grouping of customers based on their annual income and spending score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35ed4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data with cluster assignments\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels)\n",
    "plt.scatter(centers[:, 0], centers[:, 1], marker='x', color='red', label='Cluster Centers')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('K-means Clustering on PCA Data')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc26b5d",
   "metadata": {},
   "source": [
    "### 11. Evaluate the performance using silhouette score\n",
    "\n",
    "The Silhouette Score is a measure that quantifies how well each data point fits into its assigned cluster. It provides an indication of how compact and well-separated the clusters are.\n",
    "\n",
    "Its value ranges from -1 to 1. When the average Silhouette Score across all data points is closer to 1, it suggests that the clusters are well-separated and distinct. Conversely, a score closer to 0 or negative values may indicate overlapping or poorly separated clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e5522c",
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_avg = silhouette_score(X_pca, labels)\n",
    "print(f\"Silhouette Score: {silhouette_avg}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2683d664",
   "metadata": {},
   "source": [
    "### 12. More clusters\n",
    "\n",
    "If we want more clusters (for more fine-grained differentiation) while maintaining a silhouette score near the one we have we can run the same loop as we did on step 6 but plot the silhouette score for each cluster instead.\n",
    "\n",
    "There are a few caveats to this method:\n",
    "\n",
    "- Increasing the number of clusters to maximize the silhouette score can lead to overfitting. Overfitting occurs when the model becomes too complex and starts capturing noise or random variations in the data, rather than meaningful patterns.\n",
    "- The more clusters you have, the fewer samples there will be in each. Too few samples per cluster makes the model and the clusters irrelevant.\n",
    "- For every additional value of `k` (number of clusters to test) we have to train the model once more, this can be time and resource consuming on large datasets.\n",
    "- The silhouette score is just one metric for evaluating cluster quality. It may not capture all aspects of cluster quality.\n",
    "\n",
    "With that in mind, let's test anyways:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee9ef51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list to store the inertia values\n",
    "sc_values = []\n",
    "max_score = 0\n",
    "max_score_k = 0\n",
    "\n",
    "# Try different values of k\n",
    "k_values = range(5, 15)\n",
    "for k in k_values:\n",
    "    # Initialize K-means clustering algorithm\n",
    "    kmeans = KMeans(n_clusters=k, random_state=0, n_init=10)\n",
    "    \n",
    "    # Fit the model to the data\n",
    "    kmeans.fit(X_pca)\n",
    "    labels = kmeans.labels_\n",
    "    \n",
    "    # Get the silhouette score\n",
    "    sc_value = silhouette_score(X_pca, labels)\n",
    "    sc_values.append(sc_value)\n",
    "    \n",
    "    # Save the highest value\n",
    "    (max_score, max_score_k) = (sc_value, k) if sc_value > max_score else (max_score, max_score_k)\n",
    "    \n",
    "# Plot the elbow curve\n",
    "plt.plot(k_values, sc_values, marker='o')\n",
    "plt.title(f\"optimal K: {max_score_k} with score: {max_score}\")\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Inertia')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6f628a",
   "metadata": {},
   "source": [
    "Here we can see that 10 clusters might be fitting since the score is almost as much as 4 clusters. If we were to continue we would get even higher scores a 26, 30, and 37, but that's way too many clusters since our data only contains 200 samples (see for yourself with `df.describe()`)\n",
    "\n",
    "Let's plot the data with 10 clusters by doing the same as before:\n",
    "\n",
    "1. Initialize and train the model\n",
    "2. Get the labels and centers\n",
    "3. Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731b7e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=10, random_state=0, n_init=10)\n",
    "\n",
    "kmeans.fit(X_pca)\n",
    "\n",
    "labels = kmeans.labels_\n",
    "centers = kmeans.cluster_centers_\n",
    "\n",
    "# Define a list of distinct colors for the labels (0 to 9)\n",
    "label_colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k', 'purple', 'orange', 'brown']\n",
    "\n",
    "# Map each label to its corresponding color\n",
    "cluster_colors = [label_colors[label] for label in labels]\n",
    "\n",
    "scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=cluster_colors)\n",
    "\n",
    "# Create a custom color map for the colorbar\n",
    "cmap = plt.cm.colors.ListedColormap(label_colors)\n",
    "norm = plt.cm.colors.BoundaryNorm(np.arange(0, 11, 1), cmap.N)\n",
    "\n",
    "# Create a colorbar with discrete colors\n",
    "# cb = plt.colorbar(plt.cm.ScalarMappable(cmap=cmap, norm=norm), ticks=np.arange(0, 10, 1))\n",
    "\n",
    "cb = plt.colorbar(scatter, cmap=cmap, norm=norm, ticks=np.arange(0, 10, 1))\n",
    "cb.set_label(\"Cluster Labels\")\n",
    "\n",
    "plt.scatter(centers[:, 0], centers[:, 1], marker='x', color='red', label='Cluster Centers')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('K-means Clustering on PCA Data')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb9c804",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"labels\"] = labels # Add labels as new column\n",
    "df[\"pca1\"] = X_pca[:, 0]\n",
    "df[\"pca2\"] = X_pca[:, 1]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e795671",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"labels\"] == 7] # Get all label 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a41a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 df[\"Age\"].value_counts()\n",
    "#2 df[\"Age\"].describe()\n",
    "#3 df[df[\"labels\"] == 7][\"Age\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3191ff03",
   "metadata": {},
   "source": [
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efce61b2",
   "metadata": {},
   "source": [
    "## 2. Your turn!\n",
    "\n",
    "Now it's your turn to do the same thing but with *Credit card data* instead. Train a model and try to find a fitting number of clusters based on the number of samples. These are the basic steps:\n",
    "\n",
    "- Load the data\n",
    "- Extract the relevant columns\n",
    "- Scale the data\n",
    "- Reduce the dimensions using PCA\n",
    "- Plot the inertia elbow plot and make a guess for the number of clusters\n",
    "- Train your model\n",
    "- Plot the segmented data and evaluate the model\n",
    "\n",
    "The dataset can be found and downloaded here [Kaggle CreditCardData](https://www.kaggle.com/datasets/vikashsingh999/creditcarddata)\n",
    "\n",
    "### 1. Load and prepare the dataset\n",
    "\n",
    "Load the data with `pd.read_csv()`.\n",
    "\n",
    "Explore the columns with `df.info()` or on Kaggle.\n",
    "\n",
    "Use `df.drop('COLUMN_NAME', axis=1)` to remove irrelevant columns.\n",
    "\n",
    "Use `df.dropna()` to drop invalid rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "db368cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Score: 0.42092860310837976\n",
      "Optimal number of clusters (K): 11 with Silhouette Score: 0.4012100267330176\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import os\n",
    "\n",
    "# Suppress the memory leak warning for KMeans on Windows\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"sklearn\")\n",
    "\n",
    "# Set the environment variable to avoid memory leak issues with MKL (Intel Math Kernel Library)\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
    "\n",
    "# Step 1: Read the customer data from a CSV file\n",
    "df = pd.read_csv('A4_Mall_Customers.csv')\n",
    "\n",
    "# Step 2: Extract the relevant columns for clustering\n",
    "X = df[['Age', 'Annual Income (k$)', 'Spending Score (1-100)']]\n",
    "\n",
    "# Step 3: Scale the dataset using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X_scaled = scaler.transform(X)\n",
    "\n",
    "# Step 4: Principal Component Analysis (PCA) to reduce to 2D for visualization\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Step 5: Generate elbow plot to find the right number of clusters\n",
    "inertia_values = []\n",
    "k_values = range(1, 15)\n",
    "for k in k_values:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=0, n_init=10)\n",
    "    kmeans.fit(X_pca)\n",
    "    inertia_values.append(kmeans.inertia_)\n",
    "\n",
    "plt.plot(k_values, inertia_values, marker='o')\n",
    "plt.title('Elbow Plot (Customer Segmentation)')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Inertia')\n",
    "plt.show()\n",
    "\n",
    "# Based on the elbow plot, choose the optimal K (e.g., K = 4)\n",
    "K = 4\n",
    "\n",
    "# Step 6: Initialize K-means clustering algorithm\n",
    "kmeans = KMeans(n_clusters=K, random_state=0, n_init=10)\n",
    "\n",
    "# Step 7: Fit the model to the data\n",
    "kmeans.fit(X_pca)\n",
    "\n",
    "# Step 8: Get the predicted labels and cluster centers\n",
    "labels = kmeans.labels_\n",
    "centers = kmeans.cluster_centers_\n",
    "\n",
    "# Step 9: Visualize the data and clusters\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap='viridis')\n",
    "plt.scatter(centers[:, 0], centers[:, 1], marker='x', color='red', label='Cluster Centers')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('K-means Clustering on PCA Data')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Step 10: Evaluate the performance using silhouette score\n",
    "silhouette_avg = silhouette_score(X_pca, labels)\n",
    "print(f\"Silhouette Score: {silhouette_avg}\")\n",
    "\n",
    "# Step 11: Explore more clusters and evaluate silhouette score\n",
    "sc_values = []\n",
    "max_score = 0\n",
    "max_score_k = 0\n",
    "for k in range(5, 15):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=0, n_init=10)\n",
    "    kmeans.fit(X_pca)\n",
    "    labels = kmeans.labels_\n",
    "    sc_value = silhouette_score(X_pca, labels)\n",
    "    sc_values.append(sc_value)\n",
    "    if sc_value > max_score:\n",
    "        max_score, max_score_k = sc_value, k\n",
    "\n",
    "# Step 12: Plot silhouette score for different values of k\n",
    "plt.plot(range(5, 15), sc_values, marker='o')\n",
    "plt.title(f\"Optimal K: {max_score_k} with score: {max_score}\")\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.show()\n",
    "\n",
    "# Print the optimal K and its silhouette score\n",
    "print(f\"Optimal number of clusters (K): {max_score_k} with Silhouette Score: {max_score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6464e72",
   "metadata": {},
   "source": [
    "### 2. Scale the dataset\n",
    "\n",
    "Scale your data using StandardScaler to make sure that the features have similar scales.\n",
    "\n",
    "1. Create an instance of the StandardScaler class.\n",
    "2. Fit the scaler on your dataset.\n",
    "3. Transform the dataset using the fitted scaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "45f2adea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   CustomerID  Gender  Age  Annual Income (k$)  Spending Score (1-100)\n",
      "0           1    Male   19                  15                      39\n",
      "1           2    Male   21                  15                      81\n",
      "2           3  Female   20                  16                       6\n",
      "3           4  Female   23                  16                      77\n",
      "4           5  Female   31                  17                      40\n",
      "Shape of X_scaled: (200, 3)\n",
      "Mean of each feature in X_scaled: [-1.02140518e-16 -2.13162821e-16 -1.46549439e-16]\n",
      "Standard deviation of each feature in X_scaled: [1. 1. 1.]\n",
      "PCA transformed data shape: (200, 2)\n",
      "Silhouette Score: 0.42092860310837976\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Step 1: Load the dataset\n",
    "df = pd.read_csv(r'C:\\Users\\nayif\\py3b_nayef_omer\\week9\\A4_Mall_Customers.csv')  # Use the correct path\n",
    "print(df.head())\n",
    "\n",
    "# Step 2: Extract relevant columns for clustering\n",
    "X = df[['Age', 'Annual Income (k$)', 'Spending Score (1-100)']]\n",
    "\n",
    "# Step 3: Scale the dataset\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X_scaled = scaler.transform(X)\n",
    "\n",
    "# Output scaling information\n",
    "print(\"Shape of X_scaled:\", X_scaled.shape)\n",
    "print(\"Mean of each feature in X_scaled:\", X_scaled.mean(axis=0))\n",
    "print(\"Standard deviation of each feature in X_scaled:\", X_scaled.std(axis=0))\n",
    "\n",
    "# Step 4: Apply PCA (Principal Component Analysis)\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Output transformed data shape\n",
    "print(\"PCA transformed data shape:\", X_pca.shape)\n",
    "\n",
    "# Step 5: Generate elbow plot to find the optimal number of clusters\n",
    "inertia_values = []\n",
    "k_values = range(1, 15)\n",
    "for k in k_values:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=0, n_init=10)\n",
    "    kmeans.fit(X_pca)\n",
    "    inertia_values.append(kmeans.inertia_)\n",
    "\n",
    "# Plot elbow curve\n",
    "plt.plot(k_values, inertia_values, marker='o')\n",
    "plt.title('Elbow Plot (Customer Segmentation)')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Inertia')\n",
    "plt.show()\n",
    "\n",
    "# Step 6: Choose the optimal number of clusters based on the elbow plot\n",
    "K = 4  # Change this based on the elbow plot's optimal point\n",
    "\n",
    "# Step 7: Fit K-means model\n",
    "kmeans = KMeans(n_clusters=K, random_state=0, n_init=10)\n",
    "kmeans.fit(X_pca)\n",
    "\n",
    "# Step 8: Get the predicted labels and cluster centers\n",
    "labels = kmeans.labels_\n",
    "centers = kmeans.cluster_centers_\n",
    "\n",
    "# Step 9: Visualize the clusters\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap='viridis')\n",
    "plt.scatter(centers[:, 0], centers[:, 1], marker='x', color='red', label='Cluster Centers')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title(f'K-means Clustering (k={K})')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Step 10: Evaluate performance using silhouette score\n",
    "silhouette_avg = silhouette_score(X_pca, labels)\n",
    "print(f\"Silhouette Score: {silhouette_avg}\")\n",
    "\n",
    "# Step 11: Experiment with more clusters and evaluate silhouette score\n",
    "sc_values = []\n",
    "max_score = 0\n",
    "max_score_k = 0\n",
    "\n",
    "for k in range(5, 15):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=0, n_init=10)\n",
    "    kmeans.fit(X_pca)\n",
    "    labels = kmeans.labels_\n",
    "    sc_value = silhouette_score(X_pca, labels)\n",
    "    sc_values.append(sc_value)\n",
    "    if sc_value > max_score:\n",
    "        max_score = sc_value\n",
    "        max_score_k = k\n",
    "\n",
    "# Plot silhouette scores for different values of k\n",
    "plt.plot(range(5, 15), sc_values, marker='o')\n",
    "plt.title(f\"Optimal K: {max_score_k} with score: {max_score}\")\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.show()\n",
    "\n",
    "# Step 12: Visualize the final clustering result with the optimal k\n",
    "kmeans_final = KMeans(n_clusters=max_score_k, random_state=0, n_init=10)\n",
    "kmeans_final.fit(X_pca)\n",
    "labels_final = kmeans_final.labels_\n",
    "centers_final = kmeans_final.cluster_centers_\n",
    "\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels_final, cmap='viridis')\n",
    "plt.scatter(centers_final[:, 0], centers_final[:, 1], marker='x', color='red', label='Cluster Centers')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title(f'K-means Clustering (k={max_score_k})')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77c4204",
   "metadata": {},
   "source": [
    "### 3. Principal Component Analysis\n",
    "\n",
    "Use PCA to simplify and clean the data, which will make it easier to understand and find patterns.\n",
    "\n",
    "Do the following:\n",
    "\n",
    "1. Create an instance of the PCA class and specify the number of variables / principal components.\n",
    "2. Fit the PCA-object on our dataset.\n",
    "3. Transform the dataset using the fitted PCA-object (try doing this at the same time as step 2 by using `fit_transform()`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ce74ef8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Head:\n",
      "   CustomerID  Gender  Age  Annual Income (k$)  Spending Score (1-100)\n",
      "0           1    Male   19                  15                      39\n",
      "1           2    Male   21                  15                      81\n",
      "2           3  Female   20                  16                       6\n",
      "3           4  Female   23                  16                      77\n",
      "4           5  Female   31                  17                      40\n",
      "\n",
      "PCA Transformed Data:\n",
      "[[-0.61572002 -1.76348088]\n",
      " [-1.66579271 -1.82074695]\n",
      " [ 0.33786191 -1.67479894]\n",
      " [-1.45657325 -1.77242992]\n",
      " [-0.03846521 -1.66274012]]\n",
      "\n",
      "Explained Variance Ratio for Each Principal Component:\n",
      "[0.44266167 0.33308378]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Step 1: Load the dataset\n",
    "df = pd.read_csv(r'C:\\Users\\nayif\\py3b_nayef_omer\\week9\\A4_Mall_Customers.csv')  # Replace with the correct file path\n",
    "print(\"Dataset Head:\")\n",
    "print(df.head())\n",
    "\n",
    "# Step 2: Extract relevant columns for PCA (e.g., 'Age', 'Annual Income (k$)', 'Spending Score (1-100)')\n",
    "X = df[['Age', 'Annual Income (k$)', 'Spending Score (1-100)']]\n",
    "\n",
    "# Step 3: Scale the dataset (important for PCA)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Step 4: Create PCA instance and specify the number of components (e.g., 2 components)\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Step 5: Fit and transform the dataset with PCA\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Step 6: Print the transformed dataset and explained variance\n",
    "print(\"\\nPCA Transformed Data:\")\n",
    "print(X_pca[:5])  # Print first 5 rows of the transformed dataset\n",
    "\n",
    "print(\"\\nExplained Variance Ratio for Each Principal Component:\")\n",
    "print(pca.explained_variance_ratio_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b951bf8",
   "metadata": {},
   "source": [
    "### 4. Find the right amount of clusters\n",
    "\n",
    "Do the same thing as in **Part 1** to generate an elbow plot of the inertia values from the model. When the plot is displayed, set K below to the number of cluster you think will fit best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f47aab58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Step 1: Generate elbow plot to find the optimal number of clusters\n",
    "inertia_values = []\n",
    "k_values = range(1, 11)  # Testing for k = 1 to 10 clusters\n",
    "\n",
    "# Run K-means for each k and compute inertia\n",
    "for k in k_values:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=0, n_init=10)\n",
    "    kmeans.fit(X_pca)  # Use PCA-transformed data\n",
    "    inertia_values.append(kmeans.inertia_)\n",
    "\n",
    "# Step 2: Plot the elbow curve\n",
    "plt.plot(k_values, inertia_values, marker='o')\n",
    "plt.title('Elbow Plot for Optimal Number of Clusters')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Inertia')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b17eee23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After inspecting the elbow plot, set K to the optimal number of clusters\n",
    "K = 4  # Replace with your chosen value based on the elbow plot\n",
    "\n",
    "# Step 2: Fit K-means model with chosen K\n",
    "kmeans = KMeans(n_clusters=K, random_state=0, n_init=10)\n",
    "kmeans.fit(X_pca)  # Use PCA-transformed data\n",
    "\n",
    "# Step 3: Get the predicted labels and cluster centers\n",
    "labels = kmeans.labels_\n",
    "centers = kmeans.cluster_centers_\n",
    "\n",
    "# Step 4: Visualize the clusters\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap='viridis')\n",
    "plt.scatter(centers[:, 0], centers[:, 1], marker='x', color='red', label='Cluster Centers')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title(f'K-means Clustering (k={K})')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a52664d",
   "metadata": {},
   "source": [
    "### 5. Visualize and evaluate\n",
    "\n",
    "Initialize and train your model using `KMeans()` and `kmeans.fit()`. Then extract the the labels and centers and plot them using `plt.scatter()`. You can add the argument `s=1` in `plt.scatter()` to make the markers smaller if they are too large to differentiate.\n",
    "\n",
    "After showing the plot: calculate and print the silhouette score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a1bd0ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Score: 0.42092860310837976\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Train the K-means model using the chosen K\n",
    "kmeans = KMeans(n_clusters=K, random_state=0, n_init=10)\n",
    "kmeans.fit(X_pca)  # Use the PCA-transformed data\n",
    "\n",
    "# Step 2: Extract the labels and cluster centers\n",
    "labels = kmeans.labels_\n",
    "centers = kmeans.cluster_centers_\n",
    "\n",
    "# Step 3: Visualize the clusters\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, cmap='viridis', s=1)  # s=1 to make markers smaller\n",
    "plt.scatter(centers[:, 0], centers[:, 1], marker='x', color='red', label='Cluster Centers')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title(f'K-means Clustering (k={K})')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Step 4: Evaluate the clustering using silhouette score\n",
    "silhouette_avg = silhouette_score(X_pca, labels)\n",
    "print(f\"Silhouette Score: {silhouette_avg}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee3e128",
   "metadata": {},
   "source": [
    "### 6. Test more clusters\n",
    "\n",
    "Use the technique in the end of **Part 1** to see what higher values of K would result in. This dataset is about 20 times larger, which means both that each iteration of the loop (training the model and calculating the score) takes longer, but also that more clusters are \"allowed\" without having too few samples per cluster.\n",
    "\n",
    "First, plot the score for different amounts of clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0f4cb226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal number of clusters based on silhouette score: 17\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Test multiple values of K (starting from 5 to higher values)\n",
    "sc_values = []  # To store silhouette scores\n",
    "k_values = range(5, 21)  # You can try different ranges, here we try 5 to 20 clusters\n",
    "\n",
    "# Step 2: Loop over each K value\n",
    "for k in k_values:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=0, n_init=10)\n",
    "    kmeans.fit(X_pca)  # Fit the model using PCA-transformed data\n",
    "    labels = kmeans.labels_\n",
    "    \n",
    "    # Calculate silhouette score for each K\n",
    "    sc_value = silhouette_score(X_pca, labels)\n",
    "    sc_values.append(sc_value)\n",
    "\n",
    "# Step 3: Plot silhouette scores for different values of K\n",
    "plt.plot(k_values, sc_values, marker='o')\n",
    "plt.title('Silhouette Score vs Number of Clusters')\n",
    "plt.xlabel('Number of Clusters (K)')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Optional: Identify the optimal K based on the maximum silhouette score\n",
    "optimal_k = k_values[sc_values.index(max(sc_values))]\n",
    "print(f\"Optimal number of clusters based on silhouette score: {optimal_k}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b91c911",
   "metadata": {},
   "source": [
    "### 7. Visualize more clusters\n",
    "\n",
    "Select a value for K other than the one you set right after the elbow plot to train a new model on.\n",
    "\n",
    "Train the model and plot the clusters to see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "cfbe0409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Select a new value for K (other than the one chosen after the elbow plot)\n",
    "K_new = 6  # You can change this value to a different one than the initial choice from the elbow plot\n",
    "\n",
    "# Step 2: Train the KMeans model with the new value of K\n",
    "kmeans_new = KMeans(n_clusters=K_new, random_state=0, n_init=10)\n",
    "kmeans_new.fit(X_pca)  # Fit the model using PCA-transformed data\n",
    "\n",
    "# Step 3: Extract the labels and cluster centers\n",
    "labels_new = kmeans_new.labels_\n",
    "centers_new = kmeans_new.cluster_centers_\n",
    "\n",
    "# Step 4: Plot the clusters\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels_new, cmap='viridis', s=10)  # Smaller points\n",
    "plt.scatter(centers_new[:, 0], centers_new[:, 1], marker='x', color='red', label='Cluster Centers')\n",
    "plt.title(f'K-means Clustering (k={K_new})')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49540271",
   "metadata": {},
   "source": [
    "## Complete!\n",
    "\n",
    "Submit your work by pushing the changes to Github, inviting the teacher/s to your repository and submitting this link on ItsLearning under Assignment 4."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
