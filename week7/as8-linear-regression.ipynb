{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34ea0e97",
   "metadata": {},
   "source": [
    "# Assignment 3\n",
    "\n",
    "Welcome to your machine learning assignment! In this assignment, you will dive deeper into the exciting world of classification tasks using a linear regression model classifier. We will be using Python and the scikit-learn library to perform these tasks.\n",
    "\n",
    "In this assignment I will use a *House Prices dataset*, split it into training and testing sets, create a linear regression model, train the model on the training set, make predictions on the testing set, and then evaluate the performance of the model using metrics such as Mean Absolute Error (MAE), Mean Squared Error (MSE), Root Mean Squared Error (RMSE). \n",
    "\n",
    "After that, you will try to do the same thing with a database of used sold cars and train a model to make predictions of the cars prices.\n",
    "\n",
    "The House Prices dataset can be [found here](https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques/data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4ff2e5",
   "metadata": {},
   "source": [
    "## Part 1 - How it's done\n",
    "\n",
    "In this part I will demonstrate how to train a model on existing data by loading the dataset, splitting it into training and testing sets, create a classifier, train the model on the training set, make predictions on the testing set, and then evaluate the performance of the model using metrics such as Mean Absolute Error (MAE), Mean Squared Error (MSE), Root Mean Squared Error (RMSE)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b900ca",
   "metadata": {},
   "source": [
    "### 1. Import the required libraries\n",
    "\n",
    "First, I import the necessary libraries and modules that will be used in the code:\n",
    "\n",
    "- `LinearRegression` from `sklearn.linear_model` for implementing the linear regression model.\n",
    "- `mean_absolute_error` and `mean_squared_error` from `sklearn.metrics` for calculating the evaluation metrics.\n",
    "- `sqrt` from `math` to compute the square root.\n",
    "- `pandas` as pd for working with data in a tabular format.\n",
    "- `train_test_split` from `sklearn.model_selection` for splitting the dataset into training and testing sets.\n",
    "- `OneHotEncoder` from `sklearn.preprocessing` for handling other data types in our dataset, such as Neighborhood\n",
    "- `pyplot` from `matplotlib` to visualize the data and the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d93909",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from math import sqrt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9761d7d0",
   "metadata": {},
   "source": [
    "### 2. Load and prepare the dataset\n",
    "\n",
    "To preprocess the dataset, I perform the following steps:\n",
    "\n",
    "- I specify a list of relevant features that will be used for the analysis. These features include, among others, 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', and 'YearBuilt'.\n",
    "- I select only the rows that have non-null values for the specified features and the target variable 'SalePrice'. This is done using the `dropna` function, which removes rows with missing values from the DataFrame. The resulting DataFrame will contain only the selected features and the target variable.\n",
    "- After preprocessing, I split the dataset into features (`X`) and the target variable (`y`) for further analysis. The features are extracted from the DataFrame df by selecting the columns specified in the features list. The target variable 'SalePrice' is extracted separately and assigned to the variable y.\n",
    "\n",
    "By performing these steps, the code loads the dataset, preprocesses it by selecting relevant features and removing rows with missing values, and then splits it into features (X) and the target variable (y) for further analysis or modeling tasks.\n",
    "\n",
    "#### Select features\n",
    "\n",
    "To determine which features are relevant to include in the model training data, we need to consider the nature of the problem and the information provided in the dataset. It is important to select features that have a potential impact on the target variable (SalePrice) and can contribute to the prediction of house prices.\n",
    "\n",
    "- OverallQual: Overall material and finish quality of the house\n",
    "- GrLivArea: Above ground living area in square feet\n",
    "- GarageCars: Size of garage in car capacity\n",
    "- TotalBsmtSF: Total basement area in square feet\n",
    "- FullBath: Number of full bathrooms\n",
    "- YearBuilt: Original construction year of the house\n",
    "- LotArea: Total lot size in square feet\n",
    "- Neighborhood: Descriptive location within the city or area\n",
    "- YearRemodAdd: Remodeling date (same as construction date if no remodeling or additions)\n",
    "- GarageArea: Size of garage in square feet\n",
    "- OverallCond: Overall condition rating of the house\n",
    "\n",
    "These features represent different aspects of the house that could influence its sale price, such as overall quality, size, amenities, and age.\n",
    "\n",
    "To find and select features, use the different functions for exploration that we have used previously, such as `df.head()`, `df.info()`, and `df.describe()`. You can also find information about the features (columns) in the dataset by visiting the source, in this case Kaggle.com.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f779cdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('A3_house_prices_train.csv')\n",
    "\n",
    "# Print available features (columns) and their data type\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02abb523",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163fe967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the dataset\n",
    "# Select relevant features\n",
    "features = [\n",
    "    'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt', \n",
    "    'LotArea', 'YearRemodAdd', 'GarageArea', 'OverallCond', 'Neighborhood'\n",
    "]\n",
    "\n",
    "# Remove rows with missing values\n",
    "df = df[features + ['SalePrice']].dropna()\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab16519",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Neighborhood\"].value_counts()\n",
    "# df.info()\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e590590",
   "metadata": {},
   "source": [
    "### 3. Working with other data types\n",
    "\n",
    "To train the model on data like Neighborhood, which is a string instead of a number (float or int), we have to encode it using OneHotEncoder. Take a look at the columns by calling `df.info()` and `df.head()`, each 'Neighborhood'  value now has its own column, which is great for training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a666e3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of OneHotEncoder\n",
    "encoder = OneHotEncoder()\n",
    "\n",
    "# Extract the \"Neighborhood\" column from the DataFrame\n",
    "neighborhood_data = df[['Neighborhood']]\n",
    "\n",
    "# Apply OneHotEncoder to the \"Neighborhood\" column\n",
    "neighborhood_encoded = encoder.fit_transform(neighborhood_data)\n",
    "\n",
    "# Convert to a dense array\n",
    "neighborhood_encoded_dense = neighborhood_encoded.toarray()\n",
    "\n",
    "# Create a DataFrame with the encoded neighborhood data\n",
    "neighborhood_encoded_df = pd.DataFrame(neighborhood_encoded_dense, columns=encoder.get_feature_names_out(['Neighborhood']))\n",
    "\n",
    "# Concatenate the encoded neighborhood DataFrame with the original DataFrame\n",
    "df_encoded = pd.concat([df, neighborhood_encoded_df], axis=1)\n",
    "\n",
    "# Drop the original \"Neighborhood\" column from the DataFrame\n",
    "df_encoded.drop('Neighborhood', axis=1, inplace=True)\n",
    "\n",
    "# Save encoded data to the DataFrame \n",
    "df = df_encoded\n",
    "\n",
    "# Let's see the results\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5500484c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad4770a",
   "metadata": {},
   "source": [
    "### 4. Split data into training and testing sets\n",
    "\n",
    "Now I am ready to split the dataset into training and test data by using the train_test_split function. This function divides the data into training and testing sets based on a specified ratio. The training set will be used to train the model, while the testing set will be used to evaluate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a126d557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into features (X) and target variable (y)\n",
    "X = df.drop('SalePrice', axis=1)  # Exclude the 'SalePrice' column from features\n",
    "y = df['SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed84e8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074e4e87",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d13a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=12)\n",
    "\n",
    "# X_train and y_train is used to train our model\n",
    "# X_test and y_test is used to test and evaluate our model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d1ceae",
   "metadata": {},
   "source": [
    "### 5. Create an instance of the LinearRegression class and fit the model to the training data\n",
    "\n",
    "After splitting the dataset, I instantiate an instance of the LinearRegression class, which represents the linear regression model.\n",
    "\n",
    "Then, I fit the model to the training data using the `fit` method. This step trains the model by finding the best-fitting line that minimizes the difference between the predicted values and the actual target values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6e42fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create linear regression model\n",
    "regressor = LinearRegression()\n",
    "\n",
    "# Fit the model to the training data\n",
    "regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c0e161",
   "metadata": {},
   "source": [
    "### 6. Make predictions on the testing set\n",
    "\n",
    "Now that the model is trained, I can make predictions on the testing set using the predict method. This step allows me to obtain the predicted values for the target variable based on the features in the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91d7023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the testing set\n",
    "y_pred = regressor.predict(X_test)\n",
    "\n",
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fc22dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred\n",
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f06d5c2",
   "metadata": {},
   "source": [
    "### 7. Evaluate the performance of the model using MAE, MSE, and RMSE\n",
    "\n",
    "To evaluate the performance of the model, I calculate two commonly used metrics: mean absolute error (MAE) and mean squared error (MSE). These metrics measure the average absolute difference and the average squared difference, respectively, between the predicted and actual values. I import these metrics from sklearn.metrics and calculate them using the predicted values and the actual values from the testing set.\n",
    "\n",
    "Finally, I may also calculate the root mean squared error (RMSE) by taking the square root of the mean squared error. This metric provides a measure of the average magnitude of the prediction errors in the same units as the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e7beb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate evaluation metrics\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = sqrt(mse)\n",
    "\n",
    "# Print the metrics\n",
    "print(\"Mean Absolute Error (MAE):\", mae)\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "print(\"Root Mean Squared Error (RMSE):\", rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21c825a",
   "metadata": {},
   "source": [
    "### 8. Analyzing the results\n",
    "\n",
    "#### Mean Absolute Error (MAE):\n",
    "\n",
    "MAE measures the average absolute difference between the predicted and actual values.\n",
    "In this case, the MAE is approximately 19 927.\n",
    "It indicates that, on average, the model's predictions have an absolute difference of around 19 927 units (dollars) from the actual values.\n",
    "A lower MAE value suggests better accuracy, as it represents smaller prediction errors.\n",
    "\n",
    "#### Mean Squared Error (MSE):\n",
    "\n",
    "MSE calculates the average of the squared differences between the predicted and actual values.\n",
    "The MSE value is approximately 1 182 480 620\n",
    "It quantifies the average squared deviation of the model's predictions from the actual values.\n",
    "A lower MSE indicates a better fit, as it represents smaller overall prediction errors.\n",
    "\n",
    "#### Root Mean Squared Error (RMSE):\n",
    "\n",
    "RMSE is the square root of MSE and provides a measure of the average magnitude of the prediction errors.\n",
    "The RMSE value is approximately 34387.\n",
    "It represents the average difference between the predicted and actual values, in the same units as the target variable (dollars).\n",
    "Similar to MAE and MSE, a lower RMSE value indicates better accuracy and a smaller average prediction error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937ec85b",
   "metadata": {},
   "source": [
    "### 9. Visualizing the results\n",
    "\n",
    "To get a better understanding of how our model performed, we can plot the data using these plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3383d7",
   "metadata": {},
   "source": [
    "#### Scatter Plot: \n",
    "\n",
    "From a scatter plot, we can observe the relationship between two variables. If the data points form a clear pattern or follow a linear trend, it indicates a strong correlation between the variables. Additionally, we can identify any outliers or clusters in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2f1139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the sale price\n",
    "plt.scatter(range(len(y_test)), y_test, color='blue', label='Actual Sale Price')\n",
    "# Plotting the predictions\n",
    "plt.scatter(range(len(y_pred)), y_pred, color='red', label='Predicted Sale Price')\n",
    "\n",
    "plt.title('Actual Sale Price vs Predicted Sale Price')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Sale Price')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7270972",
   "metadata": {},
   "source": [
    "#### Line Plot: \n",
    "\n",
    "A line plot helps us visualize the trend or progression of a variable over time or data points. We can observe whether the variable increases, decreases, or remains relatively constant. It allows us to identify patterns, seasonality, or trends in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425d0d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(y_test)), y_test, label='Actual Sale Prices')\n",
    "plt.plot(range(len(y_test)), y_pred, label='Predicted Sale Prices')\n",
    "plt.xlabel('Data Point')\n",
    "plt.ylabel('Sale Price')\n",
    "plt.title('Actual vs Predicted Sale Prices')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7d0cc3",
   "metadata": {},
   "source": [
    "#### Histogram: \n",
    "\n",
    "By examining a histogram, we can determine the distribution of a single variable. It provides insights into the data's central tendency, spread, and shape. We can identify whether the data is normally distributed, skewed, or exhibits multiple peaks. Outliers or unusual patterns can also be detected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d389a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(y_test, bins=30, alpha=0.5, label='Actual Sale Prices')\n",
    "plt.hist(y_pred, bins=30, alpha=0.5, label='Predicted Sale Prices')\n",
    "plt.xlabel('Sale Price')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Actual and Predicted Sale Prices')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea4a25e",
   "metadata": {},
   "source": [
    "#### Box Plot: \n",
    "\n",
    "A box plot summarizes the distribution of a variable and highlights important summary statistics. It allows us to identify the median, quartiles, and potential outliers. We can assess the spread of the data, skewness, and any extreme values that fall beyond the whiskers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c5d360",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot([y_test, y_pred], labels=['Actual', 'Predicted'])\n",
    "plt.ylabel('Sale Price')\n",
    "plt.title('Box Plot of Actual and Predicted Sale Prices')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bd67e1",
   "metadata": {},
   "source": [
    "#### Residual Plot: \n",
    "\n",
    "Analyzing a residual plot helps us assess the performance of a regression model. We can determine whether the residuals exhibit any patterns or systematic deviations from the expected zero line. It helps us identify potential issues such as heteroscedasticity, non-linearity, or influential outliers in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b06a683",
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = y_test - y_pred\n",
    "plt.scatter(y_pred, residuals)\n",
    "plt.xlabel('Predicted Sale Prices')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residual Plot')\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a032ece1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Your turn\n",
    "\n",
    "Now it's your turn!\n",
    "\n",
    "The required packages are loaded an imported at the start of this document, so you're ready to get started. Your objective is to train a model on the following dataset of Ford cars: [Kaggle.com Ford Car Price Prediction](https://www.kaggle.com/datasets/adhurimquku/ford-car-price-prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4afc6d7",
   "metadata": {},
   "source": [
    "### 1. Load and prepare the dataset\n",
    "\n",
    "Load and prepare the dataset by doing the following:\n",
    "\n",
    "- Load the data with `pd.readcsv(...)`\n",
    "- Use `df.info()` and `df.describe()` to explore the dataset.\n",
    "- Select relevant features like I did in my example above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e724af03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb22689",
   "metadata": {},
   "source": [
    "### 2. Encode string data types\n",
    "\n",
    "Now you need to encode some of the data. In my example I used `OneHotEncoder` but you can use `LabelEncoder` instead since there are more columns to encode and more possible values per column. The columns you may want to encode are the columns with the datatype of `object` which you can see by using `df.info()`. THe column called `model` is one of them, but there's two more.\n",
    "\n",
    "To encode the `model` column/attribute you need to:\n",
    "\n",
    "- Initialize LabelEncoder using `LabelEncoder()` and save it to a variable (name it `encoder` for clarity).\n",
    "- Use `encoder.fit_transform(df['model'])` to encode the column. This returns an encoded column which you can save to a new column with any name you'd like. Like this: `df['model_encoded'] = encoder.fit_transform(df['model'])`\n",
    "- Drop the column you encoded from (`model`) by using `df.drop(...)`\n",
    "- Now do the same for the other two columns.\n",
    "\n",
    "When you think you're done, call `df.info()` and make sure all columns are number values such as `int32`, `int64`, or `float64`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476428a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5dd7a6",
   "metadata": {},
   "source": [
    "### 3. Split data into training and testing sets\n",
    "\n",
    "Do just as I did in my example above to split the dataset into features (`X`) and target variable (`y`) and then use `train_test_split(...)` to create training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4e51ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9517fceb",
   "metadata": {},
   "source": [
    "### 4. Create an instance of the LinearRegression class and fit the model to the training data\n",
    "\n",
    "After splitting the dataset, instantiate an instance of the LinearRegression class, which represents the linear regression model.\n",
    "\n",
    "Then fit the model to the training data using the `fit` method. This step trains the model by finding the best-fitting line that minimizes the difference between the predicted values and the actual target values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdca406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfaa5c96",
   "metadata": {},
   "source": [
    "### 5. Make predictions on the testing set\n",
    "\n",
    "Now that the model is trained, you can make predictions on the testing set using the `predict` method. This step lets you obtain the predicted values for the target variable based on the features in the testing set.\n",
    "\n",
    "Print the shape of your predictions to ensure that everything is OK. By calling `print(y_pred.shape)` you should get the result `(3594,)`. This shows you that the predictions is a one dimensional array of 3549 values, which looks right if you split your dataset test data to 20% by setting `test_size=0.2` in the previous step. \n",
    "\n",
    "> **Explanation**: The dataset consist of 17966 rows and by setting `test_size` to 0.2 you split the test data to 20% of all rows, which is about 3593 (`17966 x 0.2 = 3Â 593.2`). If you get a few more or less it's still OK since `train_test_split` will give you a random selection of rows and the exact number may differ because of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a95c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc87cd2",
   "metadata": {},
   "source": [
    "### 6. Evaluate the performance of the model using MAE, MSE, and RMSE\n",
    "\n",
    "- Use `mean_absolute_error(...)` to calculate the Mean Absolute Error (MAE)\n",
    "- Use `mean_squared_error(...)` to calculate the Mean Squared Error (MSE)\n",
    "- Use `sqrt(...)` to calculate the Root Mean Squared Error (RMSE)\n",
    "\n",
    "Print the results when you're done. Compare the results to the mean price and standard deviation of the full dataset (you get these by calling `df['price'].describe()`. Can you draw any conclusions from your results? Is your model good at predicting the prices?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a6e944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ab5a08",
   "metadata": {},
   "source": [
    "### 7. Visualize the results\n",
    "\n",
    "Follow my examples above to visualize the data using the following plot types:\n",
    "\n",
    "#### Scatter Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14579fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9a9f45",
   "metadata": {},
   "source": [
    "#### Histogram Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4e5b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efce61b2",
   "metadata": {},
   "source": [
    "#### Box Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbe0409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b008b86",
   "metadata": {},
   "source": [
    "#### Residual plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb5c8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49540271",
   "metadata": {},
   "source": [
    "## Complete!\n",
    "\n",
    "Submit your work by pushing the changes to Github, inviting the teacher/s to your repository and submitting this link on ItsLearning under Assignment 3."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
